**Multimodal Robotic Perception**

This project explores how robots can use multiple types of sensory input—like vision, sound, and language—to better perceive and act in the world.
Imagine a robot that only uses a camera to navigate—what happens in the dark, or if an object is behind a wall? Just like humans rely on a combination of sight, hearing, and touch, robots need multimodal perception to handle complex environments robustly.
