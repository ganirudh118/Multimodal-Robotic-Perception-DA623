{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3652006c",
   "metadata": {},
   "source": [
    "**Name:** Anirudh Gupta \n",
    "**Roll Number:** 210108005\n",
    "**Course:** DA623  \n",
    "**Topic:** Multimodal Robotic Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969a5f5",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " **-> Motivation**\n",
    "\n",
    "Modern robots operate in highly dynamic and unstructured environments—factories, homes, hospitals, disaster zones—where relying on just one type of sensor (like a camera) is often insufficient. Imagine trying to navigate a smoky room with only vision. Or identifying an object hidden from view but emitting a sound.\n",
    "\n",
    "This is where **multimodal robotic perception** becomes essential. Just like humans combine vision, sound, and touch to understand and act in the world, robots must learn to fuse data from multiple sources: RGB cameras, LiDARs, microphones, tactile sensors, GPS, and even natural language instructions.\n",
    "\n",
    "I chose this topic because it sits at the exciting intersection of **deep learning**, **robotics**, and **multimodal data fusion**—a field that reflects the future of intelligent machines. It's not just about sensing—it's about *understanding* the environment in a robust and flexible way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc822af",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**-> Historical Context & Related Work**\n",
    "\n",
    "Robots have long used visual sensors, but early systems were rule-based and brittle. As AI evolved, so did robotic perception. The timeline below captures key moments:\n",
    "\n",
    "- **Pre-2010s**: Most systems relied on **visual-only perception** or basic sensor fusion using Kalman filters and SLAM.\n",
    "- **2011**: Ngiam et al. introduced *Multimodal Deep Learning*, an early attempt to fuse audio-visual data for speech tasks using deep architectures.\n",
    "- **2017–2020**: Transformers emerged as a game changer in NLP and vision. Models like BERT and ViT inspired new multimodal frameworks (e.g., LXMERT, VisualBERT).\n",
    "- **2020s**: Robotics embraced multimodal deep learning. Models began integrating **language, vision, sound, and environment prediction** in real time.\n",
    "\n",
    "Key trends in recent research:\n",
    "- Multimodal fusion strategies (early, late, hybrid)\n",
    "- Use of transformers for joint representations\n",
    "- Context-aware perception driven by natural language\n",
    "- Domain transfer and zero-shot generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf86932f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**-> Summary of Reference Papers**\n",
    "\n",
    "**Paper 1: The Multi-Modal Robot Perception: Language, Information and Environment Prediction Model Based on Deep Learning**\n",
    "**Authors**: Lian Jiang, IGI Global (2024)\n",
    "\n",
    "This paper presents a deep learning model that fuses visual, linguistic, and environmental inputs to support robot understanding and navigation.\n",
    "\n",
    "**Key Architecture**:\n",
    "- CNN for visual processing.\n",
    "- LSTM for language.\n",
    "- Attention for modality alignment.\n",
    "- Prediction module for action planning.\n",
    "\n",
    "\n",
    "**Paper 2: Multimodal Fusion Transformer for Robotic Perception**\n",
    "**Authors**: Yuheng Gao, Yuhang Su (IEEE, 2024)\n",
    "\n",
    "This transformer-based model fuses image, point cloud, and audio inputs into a shared latent space using cross-modal attention.\n",
    "\n",
    "**Key Points**:\n",
    "- Robust across environments.\n",
    "- Effective cross-modal alignment.\n",
    "- Improved accuracy in perception tasks.\n",
    "\n",
    "\n",
    "**Paper 3: Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation**\n",
    "**Authors**: Delun Lai et al. (arXiv, 2025)\n",
    "\n",
    "Proposes a fusion architecture emphasizing real-time navigation and robustness.\n",
    "\n",
    "**Key Points**:\n",
    "- Lightweight visual-LiDAR feature extraction.\n",
    "- Adaptive weighted fusion.\n",
    "- Temporal modeling for dynamic perception.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae5322",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**-> Hands-on Simulation / Demo**\n",
    "A conceptual demo combining vision and language using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01741f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Image encoder using pretrained ResNet\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_model.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Text encoder using LSTM\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=100, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return hidden[-1]\n",
    "\n",
    "# Fusion Classifier\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.img_encoder = ImageEncoder()\n",
    "        self.txt_encoder = TextEncoder(vocab_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 + 128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, txt):\n",
    "        img_feat = self.img_encoder(img)\n",
    "        txt_feat = self.txt_encoder(txt)\n",
    "        fusion = torch.cat((img_feat, txt_feat), dim=1)\n",
    "        return self.classifier(fusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477b56b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**-> Reflections**\n",
    "\n",
    "**What surprised me?**\n",
    "- Natural language effectively guides attention in perception.\n",
    "- Transformers are central to modern multimodal systems.\n",
    "- Multimodal grounding is powerful but complex.\n",
    "\n",
    "**What can be improved?**\n",
    "- Real-time fusion models are still heavy.\n",
    "- Sensor alignment remains a challenge.\n",
    "- Interpretability is limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf304c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**-> References**\n",
    "\n",
    "1. Jiang, L. (2024). [The Multi-Modal Robot Perception](https://www.igi-global.com/article/the-multi-modal-robot-perception-language-information-and-environment-prediction-model-based-on-deep-learning/349987)\n",
    "2. Gao, Y., & Su, Y. (2024). [Multimodal Fusion Transformer for Robotic Perception](https://ieeexplore.ieee.org/document/10869760)\n",
    "3. Lai, D. et al. (2025). [Deep Learning-Based Multi-Modal Fusion](https://arxiv.org/abs/2504.19002)\n",
    "4. Ngiam, J. et al. (2011). Multimodal Deep Learning. ICML.\n",
    "5. Tan, H. & Bansal, M. (2019). LXMERT.\n",
    "6. Talk2Car Dataset: https://talk2car.github.io/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
